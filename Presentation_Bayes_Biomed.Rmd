---
title: "Bayesian Modelling for Biostatistics"
author: "Alexandra Posekany"
date: "WS 2020"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{xcolor}
   - \usepackage{color}
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \usepackage{amsfonts}
   - \usepackage[official,right]{eurosym}
   - \usepackage{colortbl}
   - \usepackage{ulem}
   - \usepackage{array}
   - \usepackage{xspace}
   - \usepackage{graphicx} 
   - \usepackage{fancyhdr}
   - \usepackage{subfig}
   - \usepackage{float}
   - \usepackage{multirow}
   - \usepackage{caption}
   - \usepackage{pinlabel}  
   - \usepackage[all]{xy}
   - \usepackage{alltt}
   - \usepackage{verbatim}
   - \usepackage[ngerman]{babel}
   - \usepackage[T1]{fontenc}
   - \usepackage{accents}
   - \usepackage{makecell}
   - \usepackage{eurosym}
   - \usepackage{textcomp}
   - \usepackage{multirow}
   - \usepackage{pbox}
   - \usepackage{tcolorbox}
   - \usepackage{etoolbox}
   - \usepackage{Sweave}
   - \usepackage{empheq}
   - \usepackage{multirow}
   - \usepackage{multicol}
   - \usepackage{arydshln}
   
output:
  beamer_presentation: 
  keep_tex: true
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

## define packages to install
list.of.packages <- c('ggplot2','dplyr',"tidyr",'lattice',"Pareto","HDInterval","rstanarm","rstan","rjags","bayess","MCMCpack","MASS","data.table","bayesreg","boot","MCMCvis","Rgraphviz","graph","invgamma")

## install all packages that are not already installed
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
## install INLA
#install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
#inla.upgrade() # for the stable version
#install.packages("BiocManager")

## load libraries
library(Pareto)
library(HDInterval)
library(rstanarm)
library(rjags)
library(bayess)
library(MCMCpack)
library(INLA)
library(data.table)
library(MASS)
library(bayesreg)
library(boot)
library(MCMCvis)
library(dplyr)
library(invgamma)
```


## Probabilistic Learning 

Machine Learning is based on data with one or more of the following properties:

* **Stochastic** and/or generated by a complex non-deterministic or not fully understood process
* Noisily observed
* Partially observed

Probability theory is a wide field of research focussed on expressing, modelling such uncertainties and finding appropriate data generating processes. Among those are quite prominently *Probabilistic models*. **Probabilistic** is directly connected with the term "probability". 

Some fields of unsupervised learning where no Outcome data is availble for backpropagation and Training of an algorithm, introduced probabilities rather than deterministic decisions. Among those is naive Bayesian filtering which is applied e.g. for filtering spam emails. 



## Probabilistic Modelling
 
**Probabilistic models** do not simply transform information in the data into single number outputs, but allow us to include previous knowledge and provide an information about how probable any one possible value is based on the information contained in the data. 

Therefore instead of fitting a deterministic model to the data, as is done by regression where the only stochastic part is left in the residuals, we will fit models where each part, the data, the parameters/model coefficients are inherently random. As random variables, we describe them through the means probabilities and their distributions. Based on these, algorithmic "learning" happens through "updating" information based on our observed data. 


## Introduction to Bayesian Inference

\begin{tcolorbox}{\textbf{Bayes' Theorem}}

$$
\pi(\theta|x)= \displaystyle \frac{\pi(\theta) f(x|\theta)}{\int_{\Theta}{\pi(\theta) f(x|\theta) d\theta}}.
$$

The denominator contains the \textbf{\emph{marginal distribution}}, $m(x)=\int_{\Theta}{\pi(\theta) f(x|\theta) d\theta}$ which has the normalisation purpose of assuring that \( \pi(\theta|x)\)  is a probability distribution.  

The quintessential parts of the Bayesian model are: 

\begin{itemize}
\item the \textbf{\emph{prior distribution}} $\pi(\theta)$ which expresses the uncertainty about a model parameter $\theta$ from parameter space $\Theta$;
\item the \textbf{\emph{Likelihoodfunction}} $f(x|\theta)$ which transforms the information contained in the data to the model structure and evaluates its 'fittingness', 
\item and the resulting \textbf{\emph{posterior distribution}} $\pi(\theta|x)$. 
\end{itemize}

\end{tcolorbox}


## Why bother working with distributions? 

\footnotesize 

**Prior distributions** introduce information available before the statistical analysis from any external sources into the model. This distribution basically assigns a probability to every possible value of the parameter for being a "proper" choice for the current model 

The **Likelihood function** as in all statistical models weighs how well the observed data fit into the chosen model based on for a certain choice of parameter. The dependence on the choice of parameter is relevant, as it is exactly this parameter we wish to learn about in our Bayesian model. 

In all  our previous methods for regression with and without regularisation, cross validation or Monte Carlo simulation where we aimed for estimating a single specific value of parameters/a set of model coefficients/an area under a curve etc. Contrary to all of them, probabilistic modelling aims for obtaining a **probability** of being "appropriate" for **every single value** considered. The probability distribution of every single parameter value after combining previous information (*prior*) and data information (*likelihood*) is the **posterior distribution**. 


## Updating Information

Maximum Likelihood estimation and Frequentist estimation can only use the whole sample at once and not include additional information.

Bayesian estimation allows for updating: taking the posterior after including the first observation as prior for including the second information
results in the same posterior as including both observations at once. ``\textbf{Updating}''
\begin{eqnarray*}
 p(\theta|x_1, x_2) &\propto& f(x_1, x_2|\theta) p(\theta) \\
 &\propto & f(x_2|\theta) f(x_1|\theta) p(\theta) \\
 &\propto & f(x_2|\theta) p(\theta|x_1)
\end{eqnarray*}


## Updating 

```{r echo=FALSE,fig.align='center'}
x<-seq(-6,6,by=0.05)
prior<-dnorm(x,mean=0,sd=2)
posterior1<-dnorm(x,mean=1.5,sd=1)
posterior2<-dnorm(x,mean=2.5,sd=0.5)
plot(x,prior,type="l",lty=2,main="Updating",ylim=c(0,0.8),lwd=2,col=2)
lines(x,posterior1,col=1,lwd=2)
lines(x,posterior2,col="darkgreen",lwd=2)
legend("topleft",legend=c("prior","posterior after 1 step","posterior after 2 steps"),lwd=4,col=c(2,1,"darkgreen"))
```

## Why Bayesian methods? - Advantages

\begin{itemize}
\item All uncertainty is modelled as probability, probability laws obeyed. \\
Complex hierarchical models are possible. 
\vspace{0.4cm}
\item Allows the incorporation of (prior) scientific information.
\vspace{0.4cm}
\item Appropriateness of methods does not depend on having large sample sizes (asymptotics). Relevant for medical research! 
\vspace{0.4cm}
\item Can easily obtain inferences for any quantity of interest in an intuitively interpretable manner.
	\begin{itemize}
		\item \scriptsize{Direct probability interpretations.}
		\vspace{0.4cm}
		\item \scriptsize{Prediction is straightforward. }
	\end{itemize}
\end{itemize}


## Why Bayesian methods? - Potential disadvantages

\begin{itemize}
\item Inferences depend on choice of prior and likelihood function, which may be incorrectly specified.
\vspace{0.4cm}
\item Real prior distributions can be difficult to obtain in complicated models.
\vspace{0.4cm}
\item In fact, bad prior information may be worse than no prior information.
\vspace{0.4cm}
\item Sensitivity analysis is necessary.
%\vspace{0.4cm}
%\item Lots of current work on developing reference/ignorance priors.
\end{itemize}

## The battle between Frequentist and Bayesian statistics
\scriptsize 
\begin{multicols}{2}
	\begin{itemize}
		\item
		{\bf Frequentist}
			\vspace{0.2cm}
			\begin{itemize}\scriptsize
				\item
				procedure that quantifies uncertainty (e.g., p-value, confidence interval, etc.) in terms of repeating the process that generated the data many times
				\vspace{0.2cm}
				\item
				parameters are a single fixed value and unknown
								
				\vspace{0.2cm}
				\item
				unbiased estimation 
				
				\vspace{0.2cm}
				\item
				makes probability statements only about the data 
				\item unconditional probabilities
			\end{itemize}
	\end{itemize}

\columnbreak

	\begin{itemize}
		\item
		{\bf Bayesian}	
			\vspace{0.2cm}
			\begin{itemize}\scriptsize
				\item
				procedure that represents the uncertainty about parameters with probability distributions
				\vspace{0.55cm}
				\item
				parameters are random variables and unknown
				
				\vspace{0.2cm}
				\item
				biased estimation
				
				\vspace{0.2cm}
				\item
				makes probability statements about model parameters and the data
				\item conditional probabilities
			\end{itemize}
	\end{itemize}
\end{multicols}

## Probability

\begin{itemize}
	\item 
	The main difference between classical and Bayesian statistics is the concept of probability
	
	\vspace{0.2cm}
	\begin{itemize}\scriptsize
		\item 
		from the classical point of view, probability is an ``objective'' concept
		\vspace{0.2cm}
		\item
        from the Bayesian point of view, probability is an ``subjective'' concept, as probabilities are conditional 
	\end{itemize}
	
	\vspace{0.2cm}
	\item
	Both approaches have pros and cons. However, when they are both applicable, they are unlikely to give different results.
\end{itemize}

## Revision of Distributions 

Distributions will by the building stones of Bayesian inference. We will therefore revise them - we already used them for random number generation. Now we also add the consideration for which type of scenarios which distributions are reasonable as likelihood functions and priors. 


\footnotesize
**Discrete distributions** will be in use for selected scenarios: 

* Actual categories: univariately we start with a uniform distribution and will end up with different weights for categories a-posteriori. 
* Indicators of groups: These are drawn from a Bernoulli/Binomial distribution in the dichotomous case or a multinomial distribution for more than 2 groups. 
* Counts: typically we model counts without a known maximum number of counts and apply Poisson or negative binomial distributions. 


## Applying continuous distributions

\footnotesize
For continuous distributions again several scenarios exist:

* **Modelling continuous observations**: The most frequently used distribution for modelling anything is the normal distribution due to the mean and standard deviation having a direct interpretation and simple to calculate estimators. 

   Depending on the domain of the data and the typical shape of their values other distributions become appropriate: 
   * For *skewed* data with *positive values* log-normal distributions or Gamma distributions are applied. 
   * For *bounded* data Beta distributions may be applied. 
   * For overdispersed data with heavy tails student's t distributions can be utilized. 
* **Modelling residuals**: Assuming a normal distribution of residuals is common for estimating confidence bounds of linear, generalised linear and regularised regression settings, as well as other algorithms based on least squares estimation such as LDA. This distribution assumption directly transfers the models into the Bayesian setting with the option of choosing different residual distributions. 


## Types of prior Distributions - Informative priors

\begin{itemize}
\item \textbf{\emph{Natural conjugate prior distribution}} \\
The prior has the same ''structure'' as the likelihood. \\
Therefore we can obtain a solution in closed form. \\ 
Because prior, likelihood and posterior have a common ''structure'' we are also able to interpret the prior, data and resulting posterior information in terms of this structure and the model's parameters or distributional shape and properties. \\[0.1cm]
\footnotesize
\begin{theorem}[Pitman-Koopman Lemma]
If for large enough sample size there exists a sufficient statistic of constant dimension for a family of distributions $f(.|\theta)$, then this family is exponential, if the support of $f(.|\theta)$ is independent of $\theta$.
\end{theorem}

\normalsize
\item \textbf{\emph{Elicitated prior}} \\
A prior is built 'manually' in such a way that specific 'weights' are put on specific values of the parameter based on concrete information. 

\end{itemize}

## Important Conjugate prior combinations for discrete likelihoods

\begin{itemize}
  \item \textbf{Binomial distribution} $y \sim Bin(n,p)$ \\ 
    conjugate prior for proportion $p$: $p \sim Beta(a,b)$ 
  \item \textbf{Negative Binomial distribution} $y \sim NBin(n,p)$ \\ 
    conjugate prior for proportion $p$: $p \sim Beta(a,b)$ 
  \item \textbf{Multinomial distribution} $y \sim Multi((n_1, \ldots, n_m),(p_1,\ldots,p_m))$ \\ 
    conjugate prior for proportions $p_1,\ldots,p_m$: $p \sim Dir(\alpha,(\theta_1,\ldots,\theta_m))$   
   \item \textbf{Poisson distribution} $y \sim Poi(\lambda)$ \\ 
    conjugate prior for rate $\lambda$: $\lambda \sim Ga(a,b)$ 
\end{itemize}

## Important Conjugate prior combinations for continuous likelihoods

\begin{itemize}
  \item \textbf{Normal distribution} $y \sim N(\mu,\sigma^2)$ 
    \begin{itemize}
      \item conjugate prior for mean $\mu$: $\mu \sim N(m,s^2)$
      \item conjugate prior for variance $\sigma^2$: $\sigma^2 \sim IG(a,b)$ which is equivalent to 
      \item conjugate prior for precision $\lambda = \frac{1}{\sigma^2}$: $\lambda \sim G(a,b)$
    \end{itemize}
  \item \textbf{Exponential distribution} $y \sim Ex(\lambda)$ \\ 
    conjugate prior for rate $\lambda$: $\lambda \sim Ga(a,b)$   
\end{itemize}



## Non-informative Priors

\begin{itemize}
\item \textbf{\emph{`noninformative prior'}}
\begin{itemize}
\item \textbf{\emph{Jeffrey's prior}} \\ 
Idea: invariant under diffeomorph parameter transformations\\ 
It is determined based on the Fisher information of the likelihood function. \\
$\pi_J=[det(\mathcal{I})]^{-\frac{1}{2}}$
\item \textbf{reference priors}  \\
These are specifically chosen priors which are linked to asymptotic properties of the posterior. 
\item \textbf{\emph{Maximum Entropy prior}}\\
maximizes the entropy, i.~e.~prior uncertainty about the parameter with side conditions %% Lagrange multipliers: moments, quantiles, ... equality constraints
\end{itemize}
\end{itemize}


## Simple Example - body sizes - prior

```{r echo=FALSE,results='hide'}
set.seed(4578435)
groessen<-rnorm(20,mean=178,sd=10)
paste(round(groessen,digits=0),collapse = ", ")

```

The prior information is that body sizes are normally distributed with a mean of 175 cm and a standard deviation of 14 cm.  

```{r echo=FALSE,fig.align='center'}
x<-seq(130,220,by=0.05)
y<-dnorm(x,mean = 175,sd=14)
plot(x,y,type="l",ylim=c(0,0.03),col="red",lwd=2)
abline(h=0)
```


## Simple Example - body sizes - Likelihood

\footnotesize
We start out with body sizes of persons measured in cm: `r paste(round(groessen,digits=0),collapse = ", ")`

This results in the following Likelihood function \( \prod_{i=1}^n f(x_i|\mu) \) for some selected values of \( \mu \)

```{r echo=FALSE,fig.align='center'}
mus<-seq(170,185,by=0.5)
ygroesse<-function(mu){prod(dnorm(groessen,mean = mu,sd=14))}
likelihood<-sapply(mus,ygroesse)
plot(mus,likelihood,ylim=c(0,2*10^(-33)),main="Likelihoodfunktion",col="blue",lwd=2)
abline(h=0)
```

## Simple Example - body sizes - posterior 

We obtain the posterior distribution by combining the **prior distribution**
$$
\mu \sim \mathcal{N}(175,14)
$$
$$
\color{red}{\pi(\mu) = \frac{1}{\sqrt{2\pi}14}\displaystyle e^{-\frac{1}{2} \left(\frac{\mu-175}{14}\right)^2}}
$$
and the **Likelihood function**
$$
\color{blue}{L(x|\mu) = \prod_{i=1}^n{ \frac{1}{\sqrt{2\pi}15}\displaystyle e^{-\frac{1}{2} \left(\frac{x_i-\mu}{15}\right)^2}}}
$$
calculating
$$
\pi(\mu|x) \propto \color{blue}{L(x|\mu)} \cdot \color{red}{\pi(\mu) }
$$

As the marginal distribution is simply a constant for specific data we leave it out of the calculations like all other constants which will have to be adapted in such a way that the posterior distribution is actually a probability distribution with area under the curve = 1. 

## Simple Example - body sizes - posterior 

$$
\pi(\mu|x) \propto \color{blue}{\prod_{i=1}^n{ \frac{1}{\sqrt{2\pi}15}\displaystyle e^{-\frac{1}{2} \left(\frac{x_i-\mu}{15}\right)^2}}} \cdot \color{red}{\frac{1}{\sqrt{2\pi}14}\displaystyle e^{-\frac{1}{2} \left(\frac{\mu-175}{14}\right)^2}}
$$
is expanded to \( \pi(\mu|x) \propto {\displaystyle e^{ -\frac{1}{2} \sum_{i=1}^n \left(\frac{x_i-\mu}{15}\right)^2-\frac{1}{2} \left(\frac{\mu-175}{14}\right)^2}} \)
and reordering the elements and dropping constants leads us to 
$$
\pi(\mu|x) \propto {\displaystyle e^{ -\frac{1}{2} \left( \frac{1}{15^2} \sum_{i=1}^n \left({x_i^2-2\mu x_i + \mu^2}\right)-\frac{1}{14^2} \left({\mu^2-2\mu 175+175^2}\right) \right) } }
$$
$$
\pi(\mu|x) \propto {\displaystyle e^{ -\frac{1}{2} \left( \frac{1}{15^2} \mu^2 n -2\mu\frac{1}{15^2} \sum_{i=1}^n { x_i}+\frac{1}{14^2} \mu^2-2 \mu \frac{1}{14^2} 175  \right) } }
$$
$$
\pi(\mu|x) \propto {\displaystyle e^{ -\frac{1}{2} \left(  \mu^2 \left(\frac{1}{15^2} n + \frac{1}{14^2}\right) -2\mu \left( \frac{1}{15^2} \sum_{i=1}^n { x_i} -\frac{1}{14^2} 175 \right) \right) } }
$$
The trained eye can identify the structure of a normal distribution again
$$
\color{teal}{\boldsymbol{\pi(\mu|x)}} \color{black}{\sim } \mathcal{N}\left( \frac{\color{blue}{\frac{1}{15^2} \sum_{i=1}^n { x_i}} -\color{red}{\frac{1}{14^2} 175}}{\color{blue}{\frac{1}{15^2} n } + \color{red}{\frac{1}{14^2}}}, \left(\color{blue}{\frac{1}{15^2} n} +\color{red}{ \frac{1}{14^2}}\right)^{-1} \right)
$$

## Updating 

```{r echo=FALSE,fig.align='center'}
x<-seq(130,220,by=0.05)
y<-dnorm(x,mean = 175,sd=14)
plot(x,y,type="l",ylim=c(0,0.13),col="red",lwd=3,xlab="mu",las=1)
abline(h=0)
#points(mus,likelihood*10^31,ylim=c(0,2*10^(-33)),main="Likelihoodfunktion",col="blue",lwd=2)
#axis(4,at = seq(min(likelihood)*10^31,max(likelihood)*10^31,length.out = 7),labels = seq(min(likelihood),max(likelihood),length.out = 7),las=1)
sdpost=1/sqrt(1/15^2*length(groessen)+1/14^2)
meanpost=(1/15^2*sum(groessen)+175/14^2)/(1/15^2*length(groessen)+1/14^2)
ypost<-dnorm(x,mean = meanpost,sd=sdpost)
lines(x,ypost,lwd=3,col="darkgreen")
abline(v=175,col="red",lwd=2)
abline(v=mean(groessen),col="blue",lwd=2)
abline(v=meanpost,col="darkgreen",lwd=2)
legend("topright",legend=c("prior","posterior"),lwd=4,col=c("red","darkgreen"))
legend("topleft",legend=c(paste("prior mean =",175),paste("ML estimate =",round(mean(groessen),digits=2)),paste("posterior mean =",round(meanpost,digits=2))),lwd=4,col=c("red","blue","darkgreen"))
```


## How to get information out of this posterior?

\begin{itemize}\scriptsize
		\item 
		All Inference about the parameter of interest $\boldsymbol\theta$ is based on the posterior distribution (and therefore also on the prior).

		\vspace{0.3cm}
		\item The information contained in the posterior distribution can be summarised in different ways as appropriate to the inference goal, e.g.
		\vspace{0.3cm}
		\begin{itemize}
			\begin{scriptsize}
			\item Means, standard deviations, medians. \hfill\textcolor{red}{\tiny(point estimation)}
			\vspace{0.3cm}
			\item Probability of exceeding a certain threshold, say $\theta_0$, $\Pr(\theta>\theta_0\mid\mathbf{y})$.\\ \hfill\textcolor{red}{\tiny(hypothesis tests)}
		
			\vspace{0.3cm}
			\item Credibility intervals. \hfill\textcolor{red}{\tiny(interval estimation)}
			\end{scriptsize}
		\end{itemize}

\end{itemize}


## Bayesian estimators and HPD-Intervals

\emph{posterior} distribution $\rightarrow$ many ways of defining point and interval estimators 

The basis for this are "**Loss functions**""

\begin{itemize}
 \item \textbf{posterior mean} $\leftrightarrow$ quadratic $L_2$ loss
 \item \textbf{posterior median} $\leftrightarrow$ absolute $L_1$ loss
 \item \textbf{posterior mode} $\leftrightarrow$ 0-1 loss
 \item \textbf{\underline{H}ighest \underline{P}osterior \underline{D}ensity Interval} \\
 shortest possible interval for a given coverage probability 
 \end{itemize}

## Example body sizes

```{r echo=FALSE,fig.align='center'}
x<-seq(160,190,by=0.05)
y<-dnorm(x,mean = 175,sd=14)  # prior
sdpost=1/sqrt(1/15^2*length(groessen)+1/14^2)
meanpost=(1/15^2*sum(groessen)+175/14^2)/(1/15^2*length(groessen)+1/14^2)
ypost<-dnorm(x,mean = meanpost,sd=sdpost) # posterior
plot(x,ypost,lwd=4,col="darkgreen",type="l",ylim=c(0,0.13),xlab="mu",las=1)
abline(h=0)
tabpost<-cbind(qnorm(seq(0.025,0.975,by=0.01),mean=meanpost,sd=sdpost),dnorm(qnorm(seq(0.025,0.975,by=0.01),mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost))
tabpost2<-cbind(qnorm(seq(0.005,0.995,by=0.01),mean=meanpost,sd=sdpost),dnorm(qnorm(seq(0.005,0.995,by=0.01),mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost))
polygon(rbind(c(qnorm(0.025,mean=meanpost,sd=sdpost),0),tabpost,c(qnorm(0.975,mean=meanpost,sd=sdpost),0),c(qnorm(0.025,mean=meanpost,sd=sdpost),0)),col="red",density = 20)
polygon(rbind(c(qnorm(0.005,mean=meanpost,sd=sdpost),0),tabpost2,c(qnorm(0.995,mean=meanpost,sd=sdpost),0),c(qnorm(0.005,mean=meanpost,sd=sdpost),0)),col="darkred",density = 10)
abline(v=meanpost,col="blue",lwd=2)
lines(x=c(qnorm(0.025,mean=meanpost,sd=sdpost),qnorm(0.025,mean=meanpost,sd=sdpost)),y=c(0,dnorm(qnorm(0.025,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col=2,lwd=2)
lines(x=c(qnorm(0.975,mean=meanpost,sd=sdpost),qnorm(0.975,mean=meanpost,sd=sdpost)),y=c(0,dnorm(qnorm(0.975,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col=2,lwd=2)
lines(x=c(qnorm(0.005,mean=meanpost,sd=sdpost),qnorm(0.005,mean=meanpost,sd=sdpost)),y=c(0,dnorm(qnorm(0.005,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col="darkred",lwd=2)
lines(x=c(qnorm(0.995,mean=meanpost,sd=sdpost),qnorm(0.995,mean=meanpost,sd=sdpost)),y=c(0,dnorm(qnorm(0.995,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col="darkred",lwd=2)
legend("topleft",legend=c("posterior","mean=median","HPD interval 95%","HPD interval 99%"),lwd=4,col=c("darkgreen","blue","red","darkred"))

```

## Code for graphics, posterior and HPD

\tiny
```{r echo=TRUE,fig.align='center',eval=F}
x<-seq(160,190,by=0.05)
y<-dnorm(x,mean = 175,sd=14)  # prior
sdpost=1/sqrt(1/15^2*length(groessen)+1/14^2)
meanpost=(1/15^2*sum(groessen)+175/14^2)/(1/15^2*length(groessen)+1/14^2)
ypost<-dnorm(x,mean = meanpost,sd=sdpost) # posterior
plot(x,ypost,lwd=4,col="darkgreen",type="l",ylim=c(0,0.13),xlab="mu",las=1)
abline(h=0)
tabpost<-cbind(qnorm(seq(0.025,0.975,by=0.01),mean=meanpost,sd=sdpost),
               dnorm(qnorm(seq(0.025,0.975,by=0.01),mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost))
tabpost2<-cbind(qnorm(seq(0.005,0.995,by=0.01),mean=meanpost,sd=sdpost),
                dnorm(qnorm(seq(0.005,0.995,by=0.01),mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost))
polygon(rbind(c(qnorm(0.025,mean=meanpost,sd=sdpost),0),tabpost,c(qnorm(0.975,mean=meanpost,sd=sdpost),0),c(qnorm(0.025,mean=meanpost,sd=sdpost),0)),col="red",density = 20)
polygon(rbind(c(qnorm(0.005,mean=meanpost,sd=sdpost),0),tabpost2,
              c(qnorm(0.995,mean=meanpost,sd=sdpost),0),c(qnorm(0.005,mean=meanpost,sd=sdpost),0)),
        col="darkred",density = 10)
abline(v=meanpost,col="blue",lwd=2)
lines(x=c(qnorm(0.025,mean=meanpost,sd=sdpost),qnorm(0.025,mean=meanpost,sd=sdpost)),
      y=c(0,dnorm(qnorm(0.025,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col=2,lwd=2)
lines(x=c(qnorm(0.975,mean=meanpost,sd=sdpost),qnorm(0.975,mean=meanpost,sd=sdpost)),
      y=c(0,dnorm(qnorm(0.975,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col=2,lwd=2)
lines(x=c(qnorm(0.005,mean=meanpost,sd=sdpost),qnorm(0.005,mean=meanpost,sd=sdpost)),
      y=c(0,dnorm(qnorm(0.005,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col="darkred",lwd=2)
lines(x=c(qnorm(0.995,mean=meanpost,sd=sdpost),qnorm(0.995,mean=meanpost,sd=sdpost)),
      y=c(0,dnorm(qnorm(0.995,mean=meanpost,sd=sdpost),mean=meanpost,sd=sdpost)),col="darkred",lwd=2)
legend("topleft",legend=c("posterior","mean=median","HPD interval 95%","HPD interval 99%"),lwd=4,
       col=c("darkgreen","blue","red","darkred"))

```

## Bayesian medical testing

We consider the example of the clinical trial of a simple drug which is meant to find the effectiveness of a treatment for a certain disease. In phase 1 patients are treated based previous findings in vitro and in vivo studies in animals and/or humans. In phase 2 a large number of patients receives treatment based on outcomes of the phase 1 study. 

In this sense this is the classical way of Bayesian learning through "updating" which is the reason why in the U.S. the FDA wants all trials to be evaluated according to this Bayesian scheme, as it saves patient numbers due to retaining information. In the end, our goal is to learn about the probability of the drug showing a positive effect for a patient $p$ which is basically the proportion of an underlying Binomial process.  
  

## How to do Bayesian Inference for Clinical Trials?

Let us consider $Y \sim Bin(n_1,\theta)$ with $n_1$ being the total number of patients treated in the Phase 1 trial and $\theta$ the unknown proportion of positive treatment outcomes. Here $Y$ counts the number of patients with positive outcomes. 

\vspace{0.2cm}
We note that the likelihood $$L(\theta|y_1)=\binom{n_1}{y_1}\theta^{y_1}(1-\theta)^{n_1-y_1}\propto \theta^{y_1}(1-\theta)^{n_1-y_1} = \theta^{\alpha-1}(1-\theta)^{\beta-1}$$
which is the kernel of a $Beta(\alpha,\beta)$ distribution with $\alpha=y_1+1$ and $\beta=n_1-y_1+1$.

Therefore, the parameters a and b refer to the number of paitents with positive outcomes and non-desired outcomes respectively regularised by adding 1.  
\vspace{0.2cm}
Assume that $50$ patients were treated and out of those, $30$ had a desired outcome. Then, y=30 and $n_1-y_1$=20. 

## How to intrepret conjugate priors' hyperparameters?

\vspace{0.2cm}
We will show that
	
\vspace{0.1cm}
\begin{center}
$\color{red}{\theta\sim Beta(a,b)\quad prior} \qquad ---- >  \qquad \color{teal}{\theta|y\sim Beta(a+y_1,b+n_1-y_1)\quad posterior}$
\end{center}
	
The trick here is that the prior distribution and the likelihood distribution have the same basic structure, as we have already seen in the normal distribution example for human sizes. 
\vspace{0.12cm}
Further assume that we start with **prior** information on the drug efficacy rate in a "naive" way and assume a 50% probability, then we can encode this for example as $\color{red}{Beta(5,5)}$ **prior** for $\theta$. The parameters a and b of the Beta distribution mean that out of 10 "previous" patients a=5 had a positive effect and b=5 had no desired effect. 

\vspace{0.12cm}
Under these circumstances, given the observed sample, one could learn about the proportion of patients with positive effects as an estimate of the drug efficacy rate that it follows a Beta distribution $\color{teal}{\theta|y_1\sim Beta(35,25)}$.

## Changing the prior - Getting information into the model 

If we had used a different prior distribution which carried the same information such as a $\color{red}{Beta(50,50)}$ **prior** for $\theta$. Then parameters a and b of the Beta distribution mean that out of 100 "previous" patients a=50 had a positive effect and b=50 had no desired effect.

Combining this prior information with the observed sample, one could learn about the proportion of spam emails that it follows a Beta distribution $\color{teal}{\theta|y_1\sim Beta(80,70)}$.

Obviously, we have much more prior evidence and therefore the prior observations have more influence compared to the data. This effect is called **informative** and such a prior is an **informative prior**. 

## Clinical Trial example - Updating the drug efficacy rate

```{r fig.align='center',echo=FALSE,fig.height=5,fig.width=9}
xrate<-seq(0,1,by=0.01)
prior1<-dbeta(xrate,shape1 = 5,shape2 = 5)
posterior1<-dbeta(xrate,shape1 = 35,shape2 = 25)
prior2<-dbeta(xrate,shape1 = 50,shape2 = 50)
posterior2<-dbeta(xrate,shape1 = 80,shape2 = 70)
par(mfrow=c(1,2))
plot(xrate,prior1,xlab="drug efficacy rate",ylab="density",main="Scenario 1",type="l",col="red",ylim=c(0,6.2),lwd=2)
abline(h=0)
lines(xrate,posterior1,col="darkgreen",lwd=2)
legend("topleft",legend=c("prior","posterior"),col=c("red","darkgreen"),lwd=4)
plot(xrate,prior2,xlab="drug efficacy rate",ylab="density",main="Scenario 2",type="l",col="red",ylim=c(0,9.52),lwd=2)
abline(h=0)
lines(xrate,posterior2,col="darkgreen",lwd=2)
legend("topleft",legend=c("prior","posterior"),col=c("red","darkgreen"),lwd=4)
```

## Phase 2 Trial - Informativeness as a Feature 

Similar to before $Y \sim Bin(n_2,\theta)$ with $n_2$ being the total number of patients treated in the Phase 2 trial and $\theta$ the unknown proportion of positive treatment outcomes, again $Y$ counts the number of patients with positive outcomes. 

We already saw that with a prior of $\color{red}{\theta\sim Beta(a,b)}$ we receive a posterior $\color{teal}{\theta|y_1\sim Beta(a+y_1,b+n_1-y_1)}$ after the Phase 1 of clinical trials which provides the most reasonable prior $\color{red}{\theta \sim Beta(a+y_1,b+n_1-y_1)}$ for the Phase 2 clinical trials which will produce $\color{teal}{\theta \sim Beta(a+y_1+y_2,b+(n_1-y_1)+(n_2-y_2))}$ as the appropriate posterior. 

## Clinical Relevance

Again based on the absolute numbers of patients in the Phase 2 trial ($n_2$,$y_2$) relative to the absolute numbers of patients in the Phase 1 trial ($n_1$,$y_1$) the prior will be more or less informative. In any case the Bayesian Inference allows to naturally blend Phase 1 and 2 trials (and possible lab trials) as if they had been a single larger trial, whereas this is impossible for classical inference resulting in smaller required patient number for trials to obtain similar precision of results (HPDI). 

Because of this natural feature of Bayesian updating, the FDA has decided that for ethical reasons all drug tests are to be planned and evaluated based on this Bayesian Inference Procedure. 

## R Code for scenario

\tiny
```{r fig.align='center',echo=TRUE,fig.height=5,fig.width=9,eval=FALSE}
xrate<-seq(0,1,by=0.01)
prior1<-dbeta(xrate,shape1 = 5,shape2 = 5)
posterior1<-dbeta(xrate,shape1 = 35,shape2 = 25)
prior2<-dbeta(xrate,shape1 = 50,shape2 = 50)
posterior2<-dbeta(xrate,shape1 = 80,shape2 = 70)
par(mfrow=c(1,2))
plot(xrate,prior1,xlab="spam rate",ylab="density",main="Scenario 1",
     type="l",col="red",ylim=c(0,6.2),lwd=2)
abline(h=0)
lines(xrate,posterior1,col="darkgreen",lwd=2)
legend("topleft",legend=c("prior","posterior"),col=c("red","darkgreen"),lwd=4)
plot(xrate,prior2,xlab="spam rate",ylab="density",main="Scenario 2",
     type="l",col="red",ylim=c(0,9.52),lwd=2)
abline(h=0)
lines(xrate,posterior2,col="darkgreen",lwd=2)
legend("topleft",legend=c("prior","posterior"),col=c("red","darkgreen"),lwd=4)
```


## Exercises - Task 1

\footnotesize

We recalculate the estimation of the prevalence of Covid19 in spring 2020. Samples from 1279 persons were analysed with PCR testing procedures. Out of all those not a single randomly selected person was tested positively. This obviously breaks standard testing mechanisms for extimating the proportion of infected person in Austria. 

However, additional information is available from similar tests in Germany which had a comparable behaviour of the spread of the disease at that time. In the same time span 4 positive cases out of 4068 had been found.

1. Build a Beta prior distribution for this Binomial scenario, which encodes the information of the German study. $$ \text{Reweight both parameters compared to the original counts with a factor of} \frac{1}{10}$$. 
2. Build the corresponding Binomial model for the number of people suffering from the disease based on the 1279 test. Obtain the theoretical posterior distribution for this scenario. 
3. Plot the posterior density and obtain the point estimators and 95% Highest posterior density interval of the prevalence of Covid19 (=proportion of inhabitants suffering from the disease). 
4. Explain why Statistik Austria chose this method instead of simulation-based or frequentist inference for obtaining intervals of the prevalence.

## Exercises - Task 2

\footnotesize

We revisit linear models and their residual distributions. We have already learned that the distribution of residuals is assumed to be normal. Therefore, the Bayesian linear modelling will assume a normal distribution for the data 
$$
y \sim N(x^T\beta,\sigma^2)
$$

for a single explanatory variable scenario, we will therefore consider the inference of the linear model's coefficient $\beta$ and the residual variance $\sigma^2$. 

1. Define conjugate priors for the coefficient parameter and the residual means independently. Explain how the parameters can be set to be uninformative. Compare different choice of prior parameters. 

2. Build the corresponding normal model the regression inference. Obtain the theoretical posterior distribution for both parameters separately assuming the other one to be "known". 

3. Provide the formulae for point estimators and 95% Highest posterior density interval of the regression parameters separately assuming the other one to be "known". 

4. Test this with the data from R: dataset DNase and model 
```{r eval=FALSE,echo=TRUE}
lm(conc~density,data=DNase)
```

   Compare the Bayesian against the frequentist results. 



## Stepwise Solution - Exercise 1

\footnotesize

### Beta Prior Distribution

We first build a beta prior distribution for the given binomial scenario which encodes the information of the German study. To that end, we choose a beta prior distribution with parameters $\alpha=y+1=4/10+1=1.4$ and $\beta=(n-y)/10+1=(4,068-4)/10+1=407.4$ (using the same notation as given in the lecture slides). Here, we have reweighted the German observations with a factor of $\frac{1}{10}$. We now plot the resulting prior that models the proportion of COVID positive people. We restrict the x-axis to the range $[0\%,1\%]$.


```{r}
alpha=1.4
beta=407.5
xrate=seq(0,0.01,length.out=300)
prior=dbeta(xrate, shape1=alpha, shape2=beta)
plot(xrate, prior, 
     xlab="proportion of COVID positive people",
     ylab="density",
     main="Beta prior based on German study",
     type="l",
     col="red",
     lwd=2)
```

## Theoretical Posterior Distribution

\footnotesize

We now build the corresponding binomial model for the number of people suffereing from the disease based on the 1,279 people with negative tests to obtain the theoretical posterior distribution. 
To that end, we know from the lecture that if the prior is given by $Beta(\alpha,\beta)$, the posterior is given for a binomial likelihood function by $Beta(\alpha+y,\beta+n-y)$, where $n$ is in our case 1,279 and $y$ corresponds to the number of positive tests 0. Hence, the posterior distribution is given by $Beta(\alpha,\beta+1,279)$, where $\alpha$ and $\beta$ are defined above. 

```{r}
alphaPost=alpha
betaPost=beta+1279
posterior=dbeta(xrate, alphaPost, betaPost)
plot(xrate, posterior, 
     xlab="proportion of COVID positive people",
     ylab="density",
     main="Beta posterior and prior based on German study",
     type="l",
     col="blue",
     lwd=2)

lines(xrate, prior, col="red")
legend("topright", c("posterior", "prior"), col=c("blue","red"),lwd=4)
```

## Plotting Posterior, Point Estimators and HPD 

\footnotesize

We now calculate the mean, mode and median of the posterior distribution. As we already know that the posterior follows a beta distribution, we can directly calculate those parameters by using the explicit formulas for the beta distribution. Note that for the median we take a common approximation $median=\frac{\alpha-\frac{1}{3}}{\alpha+\beta-\frac{2}{3}}$, since there is no closed-form formula of the median. Furthermore, we determine the highest posterior density (HPD) interval by using the package `HDInterval`.

```{r}

posteriorMean=alphaPost/(alphaPost+betaPost)
posteriorMode=(alphaPost-1)/(alphaPost+betaPost-2)
posteriorMedian=(alphaPost-1/3)/(alphaPost+betaPost-2/3)
posteriorhdi=hdi(qbeta, 0.95, shape1=alphaPost, shape2=betaPost) 

HDIlower=as.double(posteriorhdi["lower"])
HDIupper=as.double(posteriorhdi["upper"])

knitr::kable(cbind(c("Mean","Mode","Median","lower 95%-HPD boundary",
                     "upper 95%-HPD boundary"),
                   c(posteriorMean, 
                     posteriorMode, 
                     posteriorMedian,
                     HDIlower,
                     HDIupper)),
             col.names=c("Estimator","Posterior Point Estimate"),
             caption="Point esimators for the posterior distribution based on
             the prior that uses German COVID-19 cases")
```

## Plot of posterior density with the 95% HPD interval

```{r}
posterior=dbeta(xrate, alphaPost, betaPost)
plot(xrate, posterior, 
     xlab="proportion of COVID positive people",
     ylab="density",
     main="Beta posterior and HDI based on German study",
     type="l",
     col="blue",
     lwd=2)

plotSeq=seq(HDIlower,HDIupper,length.out=100)
polygon(x=c(HDIlower,
            plotSeq,
            HDIupper),
        y=c(0,
            dbeta(plotSeq,
                  shape1=alphaPost,
                  shape2=betaPost),
            0),col="#FF990022")

legend("topright", c("posterior", "HPD interval 95%"), col=c("blue","#FF990022"),lwd=4)
```

## Explanation why Statistik Austria chose this method instead of simulation based or frequentist inference for obtaining intervals of the prevalence

Statistik Austria had to choose a Bayesian approach, because in the Austrian data, there were no positive tests. Therefore, any frequentist inference would have resulted in point estimates of 0%. Still, we know that the true estimate of COVID-19 prevalence is positive (and not 0).

On the contrary, a Bayesian approach allows us to consider all parameters and model coefficients to be random. We can include prior information on the COVID-19 prevalance which we can update based on the available data. In our case, the German data seems to be relatively representative of Austria and can therefore be used to construct a good prior. Hence, the Bayesian approach estimates the uncertainty of the COVID-19 prevalance more sound than a simulation based or frequentist inference approach on the Austrian data.



## Stepwise Solution - Exercise 2

For Bayesian linear modelling, we assume in the following a normal distribution for the data

$$y\sim N(x \beta, \sigma^2). $$

```{r eval=TRUE,echo=FALSE,results='hide'}
plot((DNase$conc),(DNase$density))
```

## Bayesian Regession Model

\tiny

We now define conjugate priors for the coefficient parameter $\beta$ and the residual
variance $\sigma^2$ independently. We start by defining the prior of $\beta$ to be normally distributed with mean $m$ and variance $s^2$, i.e.

$$\beta\sim N(m, s^2).$$

We then define the prior of $\sigma$ to be inverse-Gamma distributed with shape parameters $a,b$, i.e.

$$\sigma^2\sim IG(a,b).$$

Equivalently, we can also consider $\lambda=\frac{1}{\sigma^2}=G(a,b)$. These priors are the conjugate priors of the likelihood $y\sim N(x \beta, \sigma^2)$. 

To make the priors informative, we would have to choose a small variance parameter $s^2$ of the normal distribution for the prior of $\beta$ (e.g. to $s^2=0.1$). We can take $m=0$ as a mean of the normal distribution (which corresponds to the case when the independent variable does not explain the dependent variable). For $\lambda$, we would have to choose $a=b$ both large. This corresponds to a prior mean of $\sigma$ being 1, while the prior has small variance (e.g. $a=b=100$, which corresponds to a variance of 1/100).

To get uniformative priors, we can increase the variance $s^2$ of the prior for $\beta$ (e.g. $s^2=1,000$). We can again take $m=0$ as a mean of the normal distribution. For $\lambda$, we can set $a=b=0.5$, which would correspond to a prior mean of $\sigma$ being 1, while the prior has comparably large variance 2 (and is in this sense uniformative).

## Posterior distribution of Regression Coefficients


\footnotesize

We now build the corresponding normal model for the regression inference. We obtain the
theoretical posterior distribution for both parameters $\beta,\lambda$ separately assuming the other one to be “known”. We start with $\beta$ and assume $\lambda$ (or equivalently $1/\sigma^2$) to be known:

\begin{align*}
\pi(\beta|x,y) & \propto \mathcal{L}(x,y|\beta)\cdot \pi(\beta)\\
& =\prod_i \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(x_i\beta-y_i)^2 \right)\cdot \frac{1}{\sqrt{2\pi}s}\exp\left(-\frac{1}{2s^2}(m-\beta)^2 \right) \\
& \propto  \exp\left(-\frac{1}{2\sigma^2}\sum_i(x_i\beta-y_i)^2 -\frac{1}{2s^2}(m-\beta)^2  \right)\\
% &= \exp\left(-\frac{1}{2\sigma^2}\sum_i(x_i^2\beta^2+y_i^2-2x_iy_i\beta) -\frac{1}{2s^2}(m^2+\beta^2-2\beta m)  \right) \\
% &\propto \exp\left(-\frac{1}{2}\left(\beta^2\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)-2\beta\cdot\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)\right)\right).
\end{align*}


By properly scaling, we therefore find

$$\pi(\beta|x,y) \sim N\left(\frac{\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)}{\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)}, \left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)^{-1}  \right). $$

## Posterior Distribution of the Model Precision

\footnotesize

We now proceed by determining the posterior distribution of $\lambda$ (and assume that $\beta$ is known):

\begin{align*}
\pi(\lambda|x,y)& \propto \mathcal{L}(x,y|\lambda)\cdot \pi(\lambda)\\
& =\prod_{i=1}^n \frac{\sqrt\lambda}{\sqrt{2\pi}}\exp\left(-\frac{\lambda}{2}(x_i\beta-y_i)^2 \right)\cdot \frac{\lambda^{a-1}\exp(-b\lambda)b^a}{\Gamma(a)} \\
& \propto \lambda^{n/2+a-1}\exp\left(-\frac{\lambda}{2}\sum_i(x_i\beta-y_i)^2-b\lambda \right)\\
& = \lambda^{n/2+a-1}\exp\left(-\lambda\left(\frac{\sum_i(x_i\beta-y_i)^2}{2}+b\right)\right).
\end{align*}

By properly scaling, we therefore find

$$\pi(\lambda|x,y) \sim G\left(\frac{n}{2}+a, \frac{\sum_i(x_i\beta-y_i)^2}{2}+b\right). $$

## Deriving HPDIs


\footnotesize

Based on the posterior distributions that we found in the previous task, we can now determine the mean, mode and median of the posterior distributions for $\beta$ and $\sigma$. We use the well-known property of normal distributions that their mean, median and mode coincide. As there is no closed-form solution for the median of the Gamma distribution, we can only provide the mode and mean as explicit formulae:

\begin{align*}
\mathbb{E}(\pi(\beta|x,y))&=median(\pi(\beta|x,y))=mode(\pi(\beta|x,y))=\frac{\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)}{\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)}\\
\mathbb{E}(\pi(\lambda|x,y))&=\frac{\frac{n}{2}+a}{\frac{\sum_i(x_i\beta-y_i)^2}{2}+b}
% mode(\pi(\lambda|x,y))&=\frac{\frac{n}{2}+a-1}{\frac{\sum_i(x_i\beta-y_i)^2}{2}+b}
\end{align*}

For the 95%-HPD interval of the posterior of $\beta$, we can use the inverse distribution function of $N\left(\frac{\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)}{\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)}, \left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)^{-1}  \right)$, i.e. the quantile function (since the normal distribution is symmetric). 

## HPD Interval for regression

\footnotesize

$$HDP_{lower}=N^{-1}\left(0.025, mean=\frac{\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)}{\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)},variance= \left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)^{-1}  \right), $$

$$HDP_{upper}=N^{-1}\left(0.975, mean=\frac{\left(\frac{\sum_ix_iy_i}{\sigma^2}+ \frac{m}{s^2}\right)}{\left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)},variance= \left(\frac{1}{s^2}+\sum_i\frac{x_i^2}{\sigma^2}\right)^{-1}  \right). $$

As the Gamma distribution is not symmetric, we cannot find the HDP of the posterior of $\sigma$ in a simple manner. Instead, we have to determine it numerically (e.g. via `hdi` of `HDInterval`).



## Bayesian Regression - practical howto

\tiny

```{r echo=TRUE}
library(bayesreg)
bayesregDNase<-bayesreg(density~I(conc^(1/2)),data=DNase)
summary(bayesregDNase)
```

## Classical Regression - a comparison

\tiny

```{r eval=TRUE,echo=TRUE}

lmDNase<-lm(density~I(conc^(1/2)),data=DNase)
summary(lmDNase)
```

## Plotting the results

```{r eval=TRUE,echo=FALSE}
newconc<-seq(min(DNase$conc),max(DNase$conc),length.out=200)
plot((DNase$conc),(DNase$density))
lines(newconc,predict(lmDNase,newdata = data.frame(conc=newconc)),col="red",lwd=6)
lines(newconc,predict(bayesregDNase,newdata = data.frame(conc=newconc)),col="darkblue",lwd=3)
legend("bottomright",legend = c("Bayes","Classical"),col=c("darkblue","red"),lwd = 5)
```


## Directed Acyclic Graphs

\begin{columns}[T] 
\begin{column}{.45\textwidth}
\color{red}\rule{\linewidth}{4pt}

\color{black}
 
\textbf{Bayesian paradigm:} \\ 
consider parameters as random variables  \\
$\rightarrow$ add prior on parameter, additional latent parameters\\[0.1cm]
Directed acyclic graph (DAG): visualisation of hierarchical model\\

\end{column}
\begin{column}{.53\textwidth}
\color{blue}\rule{\linewidth}{4pt}

\vspace*{0.3cm}

\includegraphics[width=1.1 \textwidth]{example_dag.pdf} 
\end{column}
\end{columns}


## Mathematical Model
The DAG illustrates the following distributions and their parameters: 
\begin{eqnarray*}
\textcolor{red}{ y|\mu, \lambda} & \sim& \textcolor{red}{N(\mu,\lambda^{-1})}  \\
\textcolor{red}{ \mu| m, l} & \sim& \textcolor{red}{N(m,l^{-1})} \\
\textcolor{red}{ \lambda| a, b}&\sim& \textcolor{red}{Gamma(a,b)}
\end{eqnarray*}
based on a \textcolor{blue}{Likelihood function}
\begin{eqnarray*}
\textcolor{blue}{  f({y} | \mu, \lambda) } \propto \textcolor{blue}{ \lambda^{n/2}
  \text{exp}\left\{-\frac{\lambda}{2} \sum_{i=1}^n (y_i - \mu)^2\right\} } .
\end{eqnarray*}

The directions visualise the hierarchical model structure with the data and its likelihood at the centre and priors for these parameters on the next hierarchical level. 

## Mathematical Model 

\begin{scriptsize}
The Normal- and Gamma- distribution are natural conjugate priors for any models based on normally distributed likelihood functions, such as the equivalent of t-test for Bayesian settings based on normal distributions priors and posteriors of the mean, the linear regression model and probit regression model. All have \emph{posteriors} and \emph{''full conditional''} distributions of the following forms. 
\begin{eqnarray*}
 \mu|\lambda, y &\sim&N(m^*, l^*) \\
 m^* & =& l^* \cdot ( \textcolor{red}{l \cdot m} + \textcolor{blue}{ \lambda \cdot n\cdot \bar{y}} ) \\
 l^* &=& \textcolor{red}{l} + \textcolor{blue}{n \cdot \lambda} 
\end{eqnarray*}
are the parameters of the normal posterior of the mean and 
\begin{eqnarray*}
 \lambda|\mu, y &\sim& Gamma(a^*, b^*) \\
 a^*&=& \textcolor{red}{a} + \textcolor{blue}{\frac{n}{2}} \\
 b^*&=& \textcolor{red}{b} + \textcolor{blue}{\frac{1}{2} \sum_{i=1}^n{(y_i-\mu)^2}}
\end{eqnarray*}
of the Gamma-posterior of the precision (inverse variance) where typical for Bayesian inference the \textcolor{blue}{Likelihood-based pivot} is \textbf{biased} by the \textcolor{red}{prior information}. 
\end{scriptsize}

## Posterior and Full conditionals 

In the above example we used the term posterior, although the normal posterior for the mean only applies, if the precision where known, and the Gamma posterior for the precision only applies, if the mean where known. You have dealt with these \textbf{1-dimensional posterior} scenarios in your last exercises. 

If both parameters where to be determined simultaneously, a \textbf{two-dimensional posterior} would have to be constructed. Its \textbf{marginal distribution} for every value of $\lambda$ would be the normal posterior of the mean $\mu$, while its \textbf{marginal distribution} for every value of $\mu$ would be the Gamma posterior of the precision $\lambda$. Thus, conditional on the value of $\lambda$ the 1-dimensional normal posterior for the mean is the \textbf{marginal distribution} of this 2-dimensional posterior and therefore referred to as \textbf{full conditional posterior}. These  \textbf{full conditionals} wil be the basic building stones of Gibbs samplers for MCMC simulation. 


## Revision of Monte Carlo sampling

\begin{scriptsize}
\begin{itemize}
\item Monte Carlo sampling is the predominant method of Bayesian inference because it avoids asymptotic approximations and can be used in high-dimensions.
\vspace{0.2cm}
\item The main idea is to approximate posterior summaries by drawing samples from the posterior distribution, and then using these samples to approximate posterior summaries of interest.
\vspace{0.2cm}
\item For example, if $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(S)}$ are samples from $p(\boldsymbol{\theta}\mid\mathbf{y})$, then the mean of $S$ samples can be used to approximate the posterior mean.
\vspace{0.2cm}
\item This only provides approximations of the posterior summaries of interest.
\vspace{0.2cm}
\item Many argue that this form of approximation is superior to asymptotic approximations because the Bayes CLT requires the sample size of the dataset to go to infinity and the Monte Carlo approximation requires the number of simulated values to go to infinity.
\vspace{0.2cm}
\item In most cases, $S\rightarrow\infty$ is cheaper and more realistic than $n\rightarrow\infty$.
\vspace{0.2cm}
\item But how to draw samples from some arbitrary distribution $p(\boldsymbol{\theta}\mid\mathbf{y})$?
\end{itemize}
\end{scriptsize}

## Markov Chain Monte Carlo

Basic MCMC samplers:
\begin{itemize}
\item The \textbf{Metropolis-Hastings} sampler: most universal sampling scheme
\item The \textbf{Gibbs} sampler: most commonly used, simple to understand and straightforward to calculate and implement 
\item The \textbf{Reversible Jump} sampler and the \textbf{Birth and Death} sampler: deal with varying parameter sizes
\item \textbf{Hybrid} sampler: combines at least 2 of the sampling approaches 
\end{itemize}


## Markov Chain Monte Carlo

\textbf{Idea}: to obtain samples from a distribution without this distribution being explicitly available

\textbf{Aim:} constructing an ergodic Markov chain  with stationary distribution $\xi$ in order to acquire samples from that distribution.

plug sampled values into  the Monte Carlo integration 

in a Bayesian frame work: posterior distributions often analytically intractible for complex, e.g. hierarchical Bayesian models


## Metropolis-Hastings sampler 

\textbf{Aim}: drawing $(x^{(t)})$ such that  $(x^{(t)})$, $t=0,1,\ldots$ are a Markov chain with stationary distribution being the objective \textbf{\emph{target density}} $\xi$

\textbf{Approach}: auxiliary conditional distribution, \textbf{\emph{proposal density}} $q(.|.)$ of a proposed value given the 'old' value. 

\textbf{good proposal} 
\begin{itemize}
 \item easy to simulate from or 
 \item symmetric (i.e.$q(x|y)=q(y|x)$) so that it cancels out in the acceptance probability
\end{itemize}


## Metropolis-Hastings sampler 

 \begin{itemize}
\item For $t=0$: take starting value $x_0$
\item $t>0$:
\begin{enumerate}
\item generate proposal $Y_t \sim q(y|x^{(t-1)})$
\item 
Either

\begin{tabular}{lcl}
move to proposed value &$Y_t$  & with  $\alpha(x^{(t-1)},Y_t)$ or \\[0.2cm]
stay at old value & $x^{(t-1)}$& with  $1-\alpha(x^{(t-1)},Y_t)$\\
\end{tabular}


 where \hspace{0.2cm} $\alpha(x,y)=\min{\left\{\displaystyle \frac{\xi(y)}{\xi(x)}\frac{q(x|y)}{q(y|x)} ,1 \right\}}$
 is the \emph{acceptance probability}. 
 
 The transition kernel of the Metropolis-Hastings sampler is
 \begin{eqnarray*}\label{eq:mhkernel}
 \mathcal{K}(x,y)=\alpha(x,y)q(y|x)+(1-\int{\alpha(x,y)q(y|x)dy})\delta_x(y)
 \end{eqnarray*}
\end{enumerate}
\end{itemize}

## Gibbs sampler 

\textbf{Idea}: use \textbf{full conditional distributions} 
\begin{eqnarray*}
 \xi_i(x_i|x_1,x_2,\ldots, x_{i-1}, x_{i+1}, \dots,x_p) \hspace{0.3cm} i=1,2, \ldots, p
\end{eqnarray*}
associated with the target distribution to generate samples from target distribution, if we can sample from these distributions 

Thus, unlike the MH sampler the Gibbs sampler is by definition \textbf{multidimensional}! (at least two variables are required for the conditional distributions) 

## Gibbs sampler

\begin{scriptsize}
\begin{itemize}
\item {\bf Gibbs sampling} was proposed in the early 1990s {\tiny(Geman and Geman, 1984; Gelfand and Smith, 1990)} and fundamentally changed Bayesian computing.
	\vspace{0.3cm}
	\begin{itemize}\scriptsize
		\item 
		It is attractive because it can sample from high-dimensional posteriors
		\vspace{0.3cm}
		\item 
		The main idea is to break the problem of sampling from the high-dimensional joint distribution into a series of samples from low-dimensional
		conditional distributions. {\tiny(the full conditional {\it posteriors})} \\
	\end{itemize}
\vspace{0.3cm}
\item The algorithm is straightforward:
\vspace{0.2cm}
	\begin{itemize}\scriptsize
		\item
		One begins by setting initial values for all parameters, $\boldsymbol{\theta}^{(0)}=(\theta_1^{(0)},\ldots,\theta_d^{(0)})$.
		\vspace{0.3cm}
		\item Variables are then sampled one at a time from their {\bf full conditional distributions}
			\begin{equation*}
				p(\theta_j\mid \theta_1,\ldots,\theta_{j-1},\theta_{j+1},\ldots,\theta_p,\mathbf{y}),\qquad j=1,\ldots,d.
			\end{equation*}
		\item Rather than 1 sample from $p$-dimensional joint, we make $p$-dimensional samples.
		\vspace{0.3cm}
		\item The process is repeated until the required number of samples have been generated.
	\end{itemize}
\end{itemize}
\end{scriptsize}

## Gibbs sampler
\begin{scriptsize}
\begin{itemize}
	\item Generally, given a parameter vector $\theta=(\theta_{1},\ldots,\theta_{d})$, the Gibbs sampler works as follows.
		\vspace{0.2cm}
		\begin{enumerate}
		\begin{scriptsize}
			\item[\tiny\texttt{Step 1}] Specify initial values $(\theta_{1}^{(0)},\ldots,\theta_{d}^{(0)})$.
			\vspace{0.2cm}
			\item[\tiny\texttt{Step 2}] For $t=1,\ldots,T$
			\begin{enumerate}[(a)]
			\begin{scriptsize}
				\vspace{0.2cm}
				\item[\tiny\texttt{2.1}] Simulate $\theta_{1}^{(t)}\sim p(\theta_{1}\mid\textbf{y},\theta_{2}^{(t-1)},\ldots,\theta_{d}^{(t-1)})$
				\vspace{0.2cm}
				\item[\tiny\texttt{2.2}] Simulate $\theta_{2}^{(t)}\sim p(\theta_{2}\mid\textbf{y},\theta_{1}^{(t)},\theta_{3}^{(t-1)},\ldots,\theta_{d}^{(t-1)})$
				\vspace{0.2cm}
				\item[] $\ldots$
				\vspace{0.2cm}
				\item[\tiny\texttt{2.d}]  Simulate $\theta_{d}^{(t)}\sim p(\theta_{d}\mid\textbf{y},\theta_{1}^{(t)},\theta_{2}^{(t)},\ldots,\theta_{d-1}^{(t)})$
			\end{scriptsize}
			\end{enumerate}

			\vspace{0.2cm}
			\item[\tiny\texttt{Step 3}] 
			Discard the first $k$ observations of the chain and compute the summary statistics from the {\it posterior} distribution based on 
			$(\theta_1^{(k+1)},\dots,\theta_d^{(k+1)}),\dots,(\theta_1^{(T)},\dots,\theta_d^{(T)})$
		\end{scriptsize}
		\end{enumerate}
\vspace{0.3cm}

\end{itemize}
\end{scriptsize}


## Example: Gibbs sampler 

<!--  Gibbs Sampler for bivariate normal distribution -->

<!-- Although such an algorithm would be totally superfluous for real application it is in easy to follow example for the basic explanation of how a Gibbs sampler will work.  -->

<!-- Let $x=(x_1,x_2)$ follow a bivariate normal distribution of the following type -->
<!-- \begin{eqnarray*} -->
<!-- \left. \left( \begin{array}{l} -->
<!-- X_1\\ X_2\\ \end{array} \right) \right| \rho & \sim & N_2\left( \left( \begin{array}{l} -->
<!-- \mu_1\\ \mu_2\\ \end{array} \right) ,\left(\begin{array}{cc} 1&\rho\\ -->
<!-- \rho&1 \end{array} \right) \right) -->
<!-- \end{eqnarray*} -->
<!-- Then the Gibbs algorithm will update in step $t\geq1$ as follows: -->
<!-- \begin{eqnarray*} -->
<!-- X_1^{(t)}|x_2^{(t-1)} &\sim& N(\mu_1+\rho(x_2^{(t-1)}-\mu_2),1-\rho^2) \\ -->
<!-- X_2^{(t)}|x_1^{(t)} &\sim& N(\mu_2+\rho(x_1^{(t)}-\mu_1),1-\rho^2) \\ -->
<!-- \end{eqnarray*} -->

\begin{scriptsize} 

Gibbs Sampler for normal distribution with Normal- and Gamma- conjugate priors. 
All have \emph{posteriors} and \emph{''full conditional''} distributions of the following forms. 

The full conditional distributions of

$$
\begin{matrix}
 \mu|\lambda, y &\sim&N(m^*, l^*) \\
 &&m^*  = l^* \cdot ( \textcolor{red}{l \cdot m} + \textcolor{blue}{ \lambda \cdot n\cdot \bar{y}} ) \\
 &&l^* = \textcolor{red}{l} + \textcolor{blue}{n \cdot \lambda} \\
 \lambda|\mu, y &\sim& Gamma(a^*, b^*) \\
 &&a^*= \textcolor{red}{a} + \textcolor{blue}{\frac{n}{2}} \\
 &&b^*= \textcolor{red}{b} + \textcolor{blue}{\frac{1}{2} \sum_{i=1}^n{(y_i-\mu)^2}}
 \end{matrix}
$$

are the basis for the Gibbs sampler updating

$$
\begin{matrix}
\mu^{(t)}|y, \lambda^{(t-1)}& \sim& N(m^*=l^{*} \cdot ( {l \cdot m} + { \lambda^{(t-1)} \cdot n\cdot \bar{y}}), l^*={l} + {n \cdot \lambda^{(t-1)}})\\ 
\lambda^{(t)}|\mu^{(t)}, y &\sim& Gamma(a^*={a} + {\frac{n}{2}}, b^*={b} + {\frac{1}{2} \sum_{i=1}^n{(y_i-\mu^{(t)})^2}})
\end{matrix}
$$
\end{scriptsize}


## Application of Bayesian Simulation Software with R 

[](https://www.youtube.com/playlist?list=PLM2jwuiJI2BNQvbNBCTu5xdsJc978vTwX)

### MCMC based Software/Packages

- Stan 
- BUGS (**B**ayesian Inference **U**sing **G**ibbs **S**ampling)
- JAGS (**J**ust **A**nother **G**ibbs **S**ampler)
- Other Packages with pre-implemented Samplers 

### Other Simulation Methods

- INLA (**I**ntegrated **N**ested **L**aplace **A**pproximation)
- ABC (**A**pproximate **B**ayesian **C**omputation)

## Example - Body Sizes 

\footnotesize

```{r echo=TRUE}
## Simulate Data
set.seed(45725)
N = 5000
mu = 175
sigma = 14
sigmasq = sigma^2
y = rnorm(N, mean = mu, sd = sigma)

## set up priors
mu.0 <- 0
tausq.0 <- 100
nu.0 <- .5
sigmasq.0 <- 1
rate.param <- nu.0 * sigmasq.0 / 2
shape.param <- nu.0 / 2

```

## Example - Body Sizes - Gibbs Sampler

```{r echo=TRUE}
### initialize vectors and set starting values 
num.sims <- 10000
mu.samples <- rep(0, num.sims)
sigmasq.samples <- rep(1, num.sims)

mean.y <- mean(y)
nu.n <- nu.0 + N

for (iter in 2:num.sims){
  # sample theta from full conditional
  mu.n <- (mu.0 / tausq.0 + N * mean.y / sigmasq.samples[iter - 1]) / (1 / tausq.0 + N / sigmasq.samples[iter - 1] )
  tausq.n <- 1 / (1/tausq.0 + N / sigmasq.samples[iter - 1])
  mu.samples[iter - 1] <- rnorm(1,mu.n,sqrt(tausq.n))
  
  # sample (1/sigma.sq) from full conditional
  sigmasq.n.theta <- 1/nu.n*(nu.0*sigmasq.0 + sum((y - mu.samples[iter])^2))
  sigmasq.samples[iter] <- rinvgamma(1,shape = nu.n/2, rate = nu.n*sigmasq.n.theta/2)
}
# compute posterior mean and quantiles
mean(mu.samples)
quantile(mu.samples, probs = c(.025, .975))

mean(sigmasq.samples)
quantile((sigmasq.samples)^(1/2), probs = c(.025, .975))
```

## Marginal posteriors of mu and sigma

```{r}
par(mfrow=c(1,2))
# plot marginal posterior of mu
hist(mu.samples,xlab=expression(mu),main=expression('Marginal Posterior of ' ~ mu),probability=T,xlim = c(160,190),breaks = seq(0,200,by=5),ylim=c(0,0.16))
lines(density(mu.samples))
abline(v=mu,col='red',lwd=2)
# plot marginal posterior of sigmasq
hist((sigmasq.samples)^(1/2),xlab=expression(sigma[2]),main=expression('Marginal Posterior of ' ~ sigma[2]),probability=T,xlim = c(160,190),breaks = seq(0,200,by=5),ylim=c(0,0.16))
lines(density((sigmasq.samples)^(1/2)))
abline(v=13.35^2,col='red',lwd=2)
```

## Trace Plots

```{r}
par(mfrow=c(1,2))
# plot trace plots
plot(mu.samples,type='l',ylab=expression(mu), main=expression('Trace plot for ' ~ mu))
abline(h=mu,lwd=2,col='red')
plot(sigmasq.samples^(1/2),type='l',ylab=expression(sigma[2]), main=expression('Trace plot for ' ~ sigma[2]))
abline(h=13.35^2,lwd=2,col='red')


```

## Relevance of Priors 

```{r echo=TRUE}
## set up priors
mu.0 <- 170
tausq.0 <- 100
nu.0 <- .5
sigmasq.0 <- 1
rate.param <- nu.0 * sigmasq.0 / 2
shape.param <- nu.0 / 2

```

```{r echo=FALSE}
library(invgamma)
### initialize vectors and set starting values 
num.sims <- 10000
mu.samples <- rep(0, num.sims)
sigmasq.samples <- rep(1, num.sims)

mean.y <- mean(y)
nu.n <- nu.0 + N

for (iter in 2:num.sims){
  # sample theta from full conditional
  mu.n <- (mu.0 / tausq.0 + N * mean.y / sigmasq.samples[iter - 1]) / (1 / tausq.0 + N / sigmasq.samples[iter - 1] )
  tausq.n <- 1 / (1/tausq.0 + N / sigmasq.samples[iter - 1])
  mu.samples[iter - 1] <- rnorm(1,mu.n,sqrt(tausq.n))
  
  # sample (1/sigma.sq) from full conditional
  sigmasq.n.theta <- 1/nu.n*(nu.0*sigmasq.0 + sum((y - mu.samples[iter])^2))
  sigmasq.samples[iter] <- rinvgamma(1,shape = nu.n/2, rate = nu.n*sigmasq.n.theta/2)
}
# compute posterior mean and quantiles
mean(mu.samples)
quantile(mu.samples, probs = c(.025, .975))

mean(sigmasq.samples)
quantile((sigmasq.samples)^(1/2), probs = c(.025, .975))
```

## Marginal posteriors of mu and sigma

```{r}
par(mfrow=c(1,2))
# plot marginal posterior of mu
hist(mu.samples,xlab=expression(mu),main=expression('Marginal Posterior of ' ~ mu),probability=T,xlim = c(160,190),breaks = seq(0,200,by=5),ylim=c(0,0.16))
lines(density(mu.samples))
abline(v=mu,col='red',lwd=2)
# plot marginal posterior of sigmasq
hist((sigmasq.samples)^(1/2),xlab=expression(sigma[2]),main=expression('Marginal Posterior of ' ~ sigma[2]),probability=T,xlim = c(160,190),breaks = seq(0,200,by=5),ylim=c(0,0.16))
lines(density((sigmasq.samples)^(1/2)))
abline(v=13.35^2,col='red',lwd=2)
```

## Trace Plots

```{r}
par(mfrow=c(1,2))
# plot trace plots
plot(mu.samples,type='l',ylab=expression(mu), main=expression('Trace plot for ' ~ mu))
abline(h=mu,lwd=2,col='red')
plot(sigmasq.samples^(1/2),type='l',ylab=expression(sigma[2]), main=expression('Trace plot for ' ~ sigma[2]))
abline(h=13.35^2,lwd=2,col='red')


```

## Metropolis-Hastings Sampler

```{r}


```

## Practical Example 

Data: Pima.tr2 from MASS

\footnotesize

```{r}
summary(Pima.tr2)
```


[Base Line Reference Logistic Regression of Pima Indian Diabetes Data (Kaggle)](https://www.kaggle.com/code/ksp585/pima-indian-diabetes-logistic-regression-with-r)

## Pima Indian Data 

```{r fig.width=12,fig.height=8,fig.align='center',warning=FALSE}
## put histograms on the diagonal
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs",method="spearman"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r,col=ifelse(r>=0.6,"red",ifelse(r>=0.4,"orange","black")))
}
pairs(MASS::Pima.tr2, lower.panel = panel.smooth, upper.panel = panel.cor,
diag.panel = panel.hist, las=1)

```

## Reference Analysis 

\tiny

```{r}
summary(logitPima<-glm(formula = type ~ . - npreg - skin, family = "binomial",
data = Pima.tr2))
```

```{r}
PredictTrain <- round(predict(logitPima,newdata = Pima.tr2, type = "response"),digits = 0)
table(PredictTrain,Pima.tr2$type)
```

## RStan 

Stan implements Hamiltonian Monte Carlo Simulation 

- approximate Hamiltonian dynamics simulation based on numerical integration. 

- instance of the Metropolis–Hastings algorithm, with a Hamiltonian dynamics evolution simulated using a time-reversible and volume-preserving numerical integrator (typically the leapfrog integrator) to propose a move to a new point in the state space.



## R packages related to Stan

- rstan: R Interface to Stan

- blmeco: Data Files and Functions Accompanying the Book “Bayesian Data Analysis in Ecology using R, BUGS and Stan”

- breathteststan: Stan-Based Fit to Gastric Emptying Curves

- brms: Bayesian Regression Models using Stan

- edstan: Stan Models for Item Response Theory

- idealstan: Bayesian IRT Ideal Point Models with Stan

- rstanarm: Bayesian Applied Regression Modeling via Stan

- rstansim: Simulation Studies with Stan

- rstantools: Tools for Developing R Packages Interfacing with ‘Stan’

- tmbstan: MCMC Sampling from ‘TMB’ Model Object using ‘Stan’


## Pima Indians with rstanarm

\footnotesize

```{r echo=TRUE,results='hide'}
library(rstanarm)
fit <- stan_glm(type ~  glu + bp  + bmi + ped + age,
                data = MASS::Pima.te,
                family = binomial(),
                prior_intercept = normal(0, 10),
                prior = normal(0, 2.5),
                prior_aux = cauchy(0, 2.5),
                chains = 4,
                iter = 2000,
                seed = 12345)
print (fit)
```


[Base Line Reference Logistic Regression of Pima Indian Diabetes Data with rstanarm (Kaggle)](https://www.kaggle.com/code/avehtari/bayesian-logistic-regression-with-rstanarm)


## Pima Indians with rstanarm

\footnotesize 
 
```{r}
(fit$stan_summary)[,-c(5,9,12)]
```

## 

## Integrated nested Laplace approximation (INLA) 

INLA is a method for *approximate* Bayesian inference 

- Meant for latent Gaussian models 
    very useful for models of right type with many hyperparmeters or complex hierarchical structures 

- *deterministic* algorithm that uses numerical integration to approximate the posterior distribution 

- much faster than MCMC, particularly for complex models 

## INLA

\footnotesize

```{r echo=TRUE}
library(INLA)
library(data.table)

library(MASS)
# Load the Pima Indian diabetes dataset from the mlbench package
data(Pima.tr2, package = "MASS")

# Split the data into training and testing sets using a 70/30 split
set.seed(1234)
train <- sample(nrow(Pima.tr2), 0.7 * nrow(Pima.tr2))
test <- setdiff(1:nrow(Pima.tr2), train)

y <- as.matrix(as.numeric(Pima.tr2[train, 8])-1)
x <- as.matrix(Pima.tr2[train, -8])

formula <- y ~ x

model <- inla(formula, family = "binomial", data = list(y = y, x = x), control.compute = list(dic = TRUE))

summary(model)
```



### MCMCLogit

[Bayesian Inference Logistic Regression of Pima Indian Diabetes Data with MCMCLogit](https://www.asc.ohio-state.edu/goel.1/STAT825/PROJECTS/KapatWang_Team4Report.pdf)

\footnotesize

```{r}
# Empirical Bayes - determine prior from data or pre-step analysis
Pima.tr2$diab<-(as.numeric(Pima.tr2$type)-1)
prior.data <- Pima.tr2[sample(1:dim(Pima.tr2)[1], 40), ]
prior.data.posterior <- MCMCpack::MCMClogit(diab~.-type-skin-npreg, data=prior.data, burning=1000, mcmc=100000, thin=100, seed=605874, b0=0, B0=0)
prior.mean.tr <- apply(prior.data.posterior, 2, mean)
prior.mean.tr
prior.cov.tr <- cov(prior.data.posterior)
prior.cov.tr
```


## RJags/WinBUGS

JAGS/BUGS is designed specifically with Markov Chain Monte Carlo (MCMC) methods in mind

This requires 
 
- the specification of the likelihood and related models
- the specification of conjugate priors including hyperparameters,
- definition of sampling parameters, such as simulation length (=chain length) and burn-in length and thinning of chains to reduce the Markov Chain induced dependence of samples.

## R packages related to JAGS

- rjags: R Interface to the JAGS MCMC library

- jagsUI: A Wrapper Around rjags to Streamline JAGS Analyses

- R2Jags: Providing wrapper functions to implement Bayesian analysis in JAGS.

- bayesmix: finite mixture models of univariate Gaussian distributions using JAGS

- dalmatian: Automates fitting of double GLM in JAGS.

- glmmBUGS: Generalized Linear Mixed Models with BUGS and JAGS

- HydeNet: Hybrid Bayesian Networks Using R and JAGS


## Fitting a logistic regression in JAGS

\footnotesize

```
# Description of the Bayesian model fitted in this file
# Notation:
# y_t = binomial (often binary) response variable for observation t=1,...,N
# x_{1t} = first explanatory variable for observation t
# x_{2t} = second " " " " " " " " "
# p_t = probability of y_t being 1 for observation t
# alpha = intercept term
# beta_1 = parameter value for explanatory variable 1
# beta_2 = parameter value for explanatory variable 2

# Likelihood
# y_t ~ Binomial(K,p_t), or Binomial(1,p_t) if binary
# logit(p_t) = alpha + beta_1 * x_1[t] + beta_2 * x_2[t]
# where logit(p_i) = log( p_t / (1 - p_t ))
# Note that p_t has to be between 0 and 1, but logit(p_t) has no limits

# Priors - all vague
# alpha ~ normal(0,100)
# beta_1 ~ normal(0,100)
# beta_2 ~ normal(0,100)
```

## Data Simulation 

```{r}
library(boot)
T <- 200
set.seed(123)
x_1 <- sort(runif(T, 0, 10))
x_2 <- sort(runif(T, 0, 10))
alpha <- 1
beta_1 <- 0.2
beta_2 <- -0.5
logit_p <- alpha + beta_1 * x_1 + beta_2 * x_2
p <- inv.logit(logit_p)
y <- rbinom(T, 1, p)

# effect of x_1 and x_2 on y
par(mfrow=c(1,2))
plot(x_1, y)
plot(x_2, y) # Clearly when x is high y tends to be 0


```

## Fitting Jags Model

\tiny

```{r eval=T,echo=TRUE}
library(R2jags)
library(boot) # Package contains the logit transform

# Jags code to fit the model to the simulated data
model_code <- "
model
{
  # Likelihood
  for (t in 1:T) {
    y[t] ~ dbin(p[t], K)
    logit(p[t]) <- alpha + beta_1 * x_1[t] + beta_2 * x_2[t]
  }
  # Priors
  alpha ~ dnorm(0.0,0.01)
  beta_1 ~ dnorm(0.0,0.01)
  beta_2 ~ dnorm(0.0,0.01)
}
"

# Set up the data
model_data <- list(T = T, y = y, x_1 = x_1, x_2 = x_2, K = 1)

# Choose the parameters to watch
model_parameters <- c("alpha", "beta_1", "beta_2")

# Run the model
model_run <- jags(
  data = model_data,
  parameters.to.save = model_parameters,
  model.file = textConnection(model_code),
  n.chains = 4,
  n.iter = 1000,
  n.burnin = 200,
  n.thin = 2
)
```


## Simulated results

\tiny

```{r eval=T,results='asis',echo=TRUE}
# Check the output - are the true values inside the 95% CI?
# Also look at the R-hat values - they need to be close to 1 if convergence has been achieved
print(model_run)
par(mfrow=c(1,2))
plot(model_run)
traceplot(model_run)

# Create a plot of the posterior mean regression line
post <- print(model_run)
alpha_mean <- post$mean$alpha
beta_1_mean <- post$mean$beta_1
beta_2_mean <- post$mean$beta_2

# As we have two explanatory variables I'm going to create two plots
# holding one of the variables fixed whilst varying the other
par(mfrow = c(2, 1))
plot(x_1, y)
lines(x_1,
  inv.logit(alpha_mean + beta_1_mean * x_1 + beta_2_mean * mean(x_2)),
  col = "red"
)
plot(x_2, y)
lines(x_2,
  inv.logit(alpha_mean + beta_1_mean * mean(x_1) + beta_2_mean * x_2),
  col = "red"
)

# Line for x_1 should be increasing with x_1, and vice versa with x_2
```




```{r eval=FALSE}
## Real example 
summary(MASS::Pima.tr)

# model_code <- "
# model
# {
#   # Likelihood
#     y ~ dbin(p, K)
#     logit(p) <- alpha + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3
# 
#   # Priors
#   alpha ~ dnorm(0.0,0.01)
#   beta_1 ~ dnorm(0.0,0.01)
#   beta_2 ~ dnorm(0.0,0.01)
#   beta_3 ~ dnorm(0.0,0.01)
# }
# "


# Adapted data from Royla and Dorazio (Chapter 2)
# Moth mortality data

y <- as.numeric(MASS::Pima.tr2$type)-1

# Set up the data
#real_data <- list(K = 1,y = y, x_1 = MASS::Pima.tr2$bmi, x_2 = MASS::Pima.tr2$age,x_3=MASS::Pima.tr2$ped)

T <- 12
K <- 20
y <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- c(rep("male", 6), rep("female", 6))
dose <- rep(0:5, 2)
sexcode <- as.integer(sex == "male")
# The key questions is: what are the effects of dose and sex?

# Set up the data
real_data <- list(T = T, K = K, y = y, x_1 = sexcode, x_2 = dose)

# Run the mdoel
real_data_run <- jags(
  data = real_data,
  parameters.to.save = model_parameters,
  model.file = textConnection(model_code),
  n.chains = 4,
  n.iter = 1000,
  n.burnin = 200,
  n.thin = 2
)

# Plot output
print(real_data_run)

# Create same plot as before (only for does though)
post <- print(real_data_run)
alpha_mean <- post$mean$alpha
beta_1_mean <- post$mean$beta_1
beta_2_mean <- post$mean$beta_2

# Look at effect of sex - quantified by beta_1
hist(real_data_run$BUGSoutput$sims.list$beta_1, breaks = 30)
# Seems positive - males more likely to die

# What about effect of dose?
o <- order(real_data$x_2)
par(mfrow = c(1, 1)) # Reset plots
with(real_data, plot(x_2, y, pch = sexcode)) # Data
# Males
with(
  real_data,
  lines(x_2[o],
    K * inv.logit(alpha_mean + beta_1_mean * 1 + beta_2_mean * x_2[o]),
    col = "red"
  )
)
# Females
with(
  real_data,
  lines(x_2[o],
    K * inv.logit(alpha_mean + beta_1_mean * 0 + beta_2_mean * x_2[o]),
    col = "blue"
  )
)

# Legend
legend("topleft",
  legend = c("Males", "Females"),
  lty = 1,
  col = c("red", "blue")
)
```

```{r eval=FALSE}
library(rjags)  # Load the rjags package for Bayesian inference
library(dplyr)  # Load the dplyr package for data manipulation
library(tidyr)  # Load the tidyr package for data manipulation
library(ggplot2) # Load the ggplot2 package for data visualization
library(MASS)
# Load the Pima Indian diabetes dataset from the mlbench package
data(Pima.tr2, package = "MASS")

# Split the data into training and testing sets using a 70/30 split
set.seed(1234)
train <- sample(nrow(Pima.tr2), 0.7 * nrow(Pima.tr2))
test <- setdiff(1:nrow(Pima.tr2), train)

# Define the Bayesian logistic regression model using JAGS syntax
model_string <- "
model {
  # Priors
  for (j in 1:p) {
    beta[j] ~ dnorm(0, 1e-3)
  }
  sigma ~ dunif(0, 100)

  # Likelihood
  for (i in 1:n) {
    y[i] ~ dbern(plogis(mu[i]))
    mu[i] <- inprod(beta[], X[i,])
  }

  # Derived quantities
  odds_ratio <- exp(beta)
}

"

# Convert train and test to matrices
train <- as.matrix(PimaIndiansDiabetes2[train,])
test <- as.matrix(PimaIndiansDiabetes2[test,])

# Compile the model using JAGS and pass in the training data as a list
model <- jags.model(textConnection(model_string), data = list(X = train[,-9], y = train[,9], n = nrow(train), p = ncol(train)-1), n.chains = 3)

# Burn-in and sample from the posterior distribution using MCMC
update(model, n.iter = 1000)
samples <- coda.samples(model, variable.names = c("beta", "odds_ratio", "sigma"), n.iter = 5000)

# Summarize the posterior distribution of the coefficients using coda functions
summary(samples$beta)

```

```{r eval=FALSE}
# Load required packages
library(rjags)
library(MCMCvis)
library(dplyr)

# Load the Pima Indian diabetes dataset
PimaIndiansDiabetes <- MASS::Pima.tr2

# Data preparation
diabetes <- PimaIndiansDiabetes[,-8]
diabetes$diabetes <- as.factor(PimaIndiansDiabetes$type)  # Convert the outcome variable to a factor

# Specify the Bayesian logistic regression model
model_code <- "
model {
  # Prior distributions
  beta0 ~ dnorm(0, 0.001)  # Prior for intercept
  for (j in 1:p) {
    beta[j] ~ dnorm(0, 0.001)  # Prior for coefficients
  }
  
  # Likelihood
  for (i in 1:n) {
    y[i] ~ dbern(p[i])
    logit(p[i]) <- beta0 + inprod(X[i,], beta[])
  }
  
  # Model parameters
  p[1:n] <- 1 / (1 + exp(-mu[1:n]))
  mu[1:n] <- X[1:n,] %*% beta[]
}
"

# Define the data and parameters for JAGS
data_list <- list(
  X = as.matrix(diabetes[, -8]),  # Independent variables
  y = as.integer(diabetes$diabetes) - 1,  # Dependent variable (0 or 1)
  n = nrow(diabetes),  # Number of observations
  p = ncol(diabetes) - 1  # Number of predictors
)

# Specify the JAGS model
model <- jags.model(textConnection(model_code), data = data_list, n.chains = 3)

# Burn-in and sampling
burnin <- 1000
n.iter <- 5000
n.thin <- 10

# Run the MCMC chains
samples <- coda.samples(model, variable.names = c("beta0", "beta"), n.iter = n.iter,
                        thin = n.thin, progress.bar = "text")

# Summarize the posterior distribution
summary(samples)

# Plot the posterior distribution
plot(samples)

# Diagnose the MCMC chains
autocorr.diag(samples)
gelman.diag(samples)

```



## Discrete Distributions
\scriptsize
\begin{description}
  \item[Uniform distribution:] $\mathbb{P}(x| x_{min}, x_{min}) = \displaystyle \frac{1}{x_{max}-x_{min}}$ is the probability of the uniform distribution denoted as $U(x_{min},x_{max})$, where $x_{min} \in \mathbb{R}$ and $x_{max} \in \mathbb{R}$.
 \item[Binomial distribution:] $\mathbb{P}(x| p, n) = \displaystyle \left( \begin{array}{c} n\\x  \end{array} \right) \cdot p^x \cdot (1-p)^{n-x}$ is the probability of the Binomial distribution denoted as $Bin(n,p)$, where $n \in \mathbb{N}$ is the total number of trials and $p \in [0,1]$ the probability of success.
 \item[Hypergeometric distribution:] $\mathbb{P}(x| N, n, I) = \displaystyle \frac{\left( \begin{array}{c} I\\x  \end{array} \right) \cdot \left( \begin{array}{c} N-I\\ n-x  \end{array} \right)  }{\left( \begin{array}{c} N\\n  \end{array} \right) } $ is the probability of the Hypergeometric distribution denoted as $Hyp(N,n,I)$, where $N \in \mathbb{N}$ is the total number of observables, $n \in \mathbb{N}$ is the numerb of observables drawn for the sample and $I \in \mathbb{N}$ the total number of interesting units.
\end{description}

## Discrete Distributions I - Visualisation

```{r discretedistributionsI,echo=FALSE,fig.align='center',fig.height=10,fig.width=12,out.width=250}
x1<-seq(0,10,by=1)
x2<-seq(0,20,by=1)
yunif1<-dunif(x2,min=1,max=8)
yunif11<-punif(x2,min=1,max=8); names(yunif11)<-x2
yunif2<-dunif(x2,min=5,max=15)
yunif21<-punif(x2,min=5,max=15); names(yunif21)<-x2
ybinom1<-dbinom(x2,size = 10,prob = 0.3)
ybinom11<-pbinom(x2,size = 10,prob = 0.3);names(ybinom11)<-x2
ybinom2<-dbinom(x2,size = 10,prob=0.8)
ybinom21<-pbinom(x2,size = 10,prob=0.8);names(ybinom21)<-x2
ybinom3<-dbinom(x2,size = 20,prob=0.8)
ybinom31<-pbinom(x2,size = 20,prob=0.8);names(ybinom31)<-x2
yhyper1<-dhyper(x2,m = 3,n = 20,k = 10)
yhyper11<-phyper(x2,m = 3,n = 20,k = 10);names(yhyper11)<-x2
yhyper2<-dhyper(x2,m = 8,n = 20,k = 10)
yhyper21<-phyper(x2,m = 8,n = 20,k = 10);names(yhyper21)<-x2
yhyper3<-dhyper(x2,m = 14,n = 20,k = 16)
yhyper31<-phyper(x2,m = 14,n = 20,k = 16);names(yhyper31)<-x2

names(x1)<-0:10
names(x2)<-0:20
par(mfrow=c(2,3))
plot(x2,yunif1,type="p",las=1,col="darkgreen",lwd=3,main="uniform distribution",ylab="",xlab="x")
points(x2,yunif2,col="blue",lwd=3)
legend("topright",legend = c("Unif(1,8)","Unif(5,15)"),col=c("darkgreen","blue"),lwd=5)
plot(x2,ybinom1,type="p",las=1,col="darkgreen",lwd=3,main="Binomial distribution",ylab="",xlab="x")
points(x2,ybinom2,col="blue",lwd=3)
points(x2,ybinom3,col="red",lwd=3)
legend("topright",legend = c("Bin(0.3,10)","Bin(0.8,10)","Bin(0.8,20)"),col=c("darkgreen","blue","red"),lwd=5)
plot(x2,yhyper1,type="p",las=1,col="darkgreen",lwd=3,main="Hypergeometric distribution",ylab="",xlab="x")
points(x2,yhyper2,col="blue",lwd=3)
points(x2,yhyper3,col="red",lwd=3)
legend("topright",legend = c("Hyp(3,10,20)","Hyp(8,10,20)","Hyp(14,16,20)"),col=c("darkgreen","blue","red"),lwd=5)

## CDF
plot.ecdf(stepfun(c(x2),c(yunif11,1)),col="darkgreen",main="CDF")
plot.ecdf(stepfun(c(x2),c(yunif21,1)),col="blue",add=TRUE)
legend("bottomright",legend = c("Unif(1,8)","Unif(5,15)"),col=c("darkgreen","blue"),lwd=5)
plot.ecdf(stepfun(c(x2),c(ybinom11,1)),col="darkgreen",main="CDF")
plot.ecdf(stepfun(c(x2),c(ybinom21,1)),col="blue",add=TRUE)
plot.ecdf(stepfun(c(x2),c(ybinom31,1)),col="red",add=TRUE)
legend("bottomright",legend = c("Bin(0.3,10)","Bin(0.8,10)","Bin(0.8,20)"),col=c("darkgreen","blue","red"),lwd=5)
plot.ecdf(stepfun(c(x2),c(yhyper11,1)),col="darkgreen",main="CDF")
plot.ecdf(stepfun(c(x2),c(yhyper21,1)),col="blue",add=TRUE)
plot.ecdf(stepfun(c(x2),c(yhyper31,1)),col="red",add=TRUE)
legend("bottomright",legend = c("Hyp(3,10,20)","Hyp(8,10,20)","Hyp(14,16,20)"),col=c("darkgreen","blue","red"),lwd=5)

```


## Discrete Distributions

\begin{description}
 \item[Negative Binomial distribution:] $\mathbb{P}(x| p, n) = \displaystyle \left( \begin{array}{c} n + x - 1 \\ x  \end{array} \right) \cdot p^x \cdot (1-p)^{n}$ is the probability of the Negative Binomial distribution denoted as $NB(n,p)$, where $n \in \mathbb{N}$ is the total number of failures out of n+x trials until x successes are reached and $p \in [0,1]$ the probability of success.
 \item[Poisson distribution:] $\mathbb{P}(x|\lambda) = \displaystyle \frac{\lambda^x}{x!} \displaystyle e^{-\lambda \cdot x} $ is the probability of the Poisson distribution denoted as $Poi(\lambda)$, where $\lambda \in \mathbb{R}^+$ is the mean number of occurrences of an interesting event per reference unit.
\end{description}

## Discrete Distributions II - Visualisation

```{r discretedistributionsII,echo=FALSE,fig.align='center',fig.height=10,fig.width=10,out.width=250}
x1<-seq(0,10,by=1)
x2<-seq(0,20,by=1)
ypoisson1<-dpois(x2,lambda = 2)
ypoisson11<-ppois(x2,lambda=2); names(ypoisson11)<-x2
ypoisson2<-dpois(x2,lambda=5)
ypoisson21<-ppois(x2,lambda=5); names(ypoisson21)<-x2
ynegbinom1<-dnbinom(x2,size = 10,prob = 0.3)
ynegbinom11<-pnbinom(x2,size = 10,prob = 0.3);names(ynegbinom11)<-x2
ynegbinom2<-dnbinom(x2,size = 10,prob=0.8)
ynegbinom21<-pnbinom(x2,size = 10,prob=0.8);names(ynegbinom21)<-x2
ynegbinom3<-dnbinom(x2,size = 20,prob=0.8)
ynegbinom31<-pnbinom(x2,size = 20,prob=0.8);names(ynegbinom31)<-x2

names(x1)<-0:10
names(x2)<-0:20
par(mfrow=c(2,2))
plot(x2,ypoisson1,type="p",las=1,col="darkgreen",lwd=3,main="Poisson distribution",ylab="",xlab="x")
points(x2,ypoisson2,col="blue",lwd=3)
legend("topright",legend = c("Poi(2)","Poi(5)"),col=c("darkgreen","blue"),lwd=5)
plot(x2,ynegbinom1,type="p",las=1,col="darkgreen",lwd=3,main="Negative Binomial distribution",ylab="",xlab="x",ylim=c(0,max(ynegbinom1,ynegbinom2,ynegbinom3)))
points(x2,ynegbinom2,col="blue",lwd=3)
points(x2,ynegbinom3,col="red",lwd=3)
legend("topright",legend = c("NBin(0.3,10)","NBin(0.8,10)","NBin(0.8,20)"),col=c("darkgreen","blue","red"),lwd=5)

## CDF
plot.ecdf(stepfun(c(x2),c(ypoisson11,1)),col="darkgreen",main="CDF")
plot.ecdf(stepfun(c(x2),c(ypoisson21,1)),col="blue",add=TRUE)
legend("bottomright",legend = c("Poi(2)","Poi(5)"),col=c("darkgreen","blue"),lwd=5)
plot.ecdf(stepfun(c(x2),c(ybinom11,1)),col="darkgreen",main="CDF")
plot.ecdf(stepfun(c(x2),c(ybinom21,1)),col="blue",add=TRUE)
plot.ecdf(stepfun(c(x2),c(ybinom31,1)),col="red",add=TRUE)
legend("bottomright",legend = c("NBin(0.3,10)","NBin(0.8,10)","NBin(0.8,20)"),col=c("darkgreen","blue","red"),lwd=5)

```


## Continuous Distributions I

\begin{description}
  \item[Normal distribution:] $f(x| \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$ is the density of the normal distribution denoted as $N(\mu,\sigma^2)$, where $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$.
  \item[student's t-distribution:] $f(x| \nu) = \frac{\Gamma((\nu+1)/2)}{\sqrt{2\nu}\Gamma(\nu/2)} \left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}$ is the density of the t-distribution with $\nu$ degrees of freedom, denoted as $t_\nu$, where $\nu \in \mathbb{N}^+$. For $\nu = 1$ the distribution is called Cauchy distribution.
  \item[Cauchy distribution:] $f(x| \gamma) = \displaystyle \frac{1}{ \pi \gamma  \left( 1+\frac{x^2}{\gamma^2}\right)}$ is the density of the Cauchy-distribution with scale parameter $\gamma$. 
  \item[Beta distribution:] $f(x| \alpha, \beta)= \frac{\Gamma(\alpha +\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1} (1-x)^{\beta -1}$ is the density of the Beta distribution denoted as $Be(\alpha,\beta)$, where $x \in [0,1]$, $\alpha \in \mathbb{R}$ and $\beta \in \mathbb{R}^+$.
\end{description}

## Continuous Distributions I - Visualisation

```{r distributionsI,echo=FALSE,fig.align='center',fig.height=10,fig.width=10,out.width=250}
x1<-seq(-5,5,by=0.02)
x2<-seq(0,1,by=0.01)
ynorm<-dnorm(x1)
ynorm2<-dnorm(x1,sd=0.5)
yt2<-dt(x1,df = 2)
yt10<-dt(x1,df = 10)
ycauchy<-dcauchy(x1)
ycauchy2<-dcauchy(x1,scale=2)
ybeta<-dbeta(x2,shape1=1,shape2 = 1)
ybeta2<-dbeta(x2,shape1=0.5,shape2 = 0.5)
ybeta3<-dbeta(x2,shape1=2,shape2 = 2)
par(mfrow=c(2,2))
plot(x1,ynorm2,type="l",las=1,col="darkgreen",lwd=3,main="normal distribution",ylab="",xlab="x")
lines(x1,ynorm,col="blue",lwd=3)
legend("topright",legend = c("sd=1","sd=0.5"),col=c("darkgreen","blue"),lwd=5)
plot(x1,yt10,type="l",las=1,col="blue",lwd=3,main="student's t distribution",ylab="",xlab="x")
lines(x1,yt2,col="darkgreen",lwd=3)
legend("topright",legend = c("nu=2","nu=10"),col=c("darkgreen","blue"),lwd=5)
plot(x1,ycauchy,type="l",las=1,col="darkgreen",lwd=3,main="Cauchy distribution",ylab="",xlab="x")
lines(x1,ycauchy2,col="blue",lwd=3)
legend("topright",legend = c("scale=1","scale=2"),col=c("darkgreen","blue"),lwd=5)
plot(x2,ybeta,type="l",las=1,col="darkgreen",lwd=3,ylim=c(0,1.6),main="Beta distribution",ylab="",xlab="x")
lines(x2,ybeta2,col="blue",lwd=3)
lines(x2,ybeta3,col="darkred",lwd=3)
legend("bottom",legend = c("a=b=1","a=b=0.5","a=b=2"),col=c("darkgreen","blue","darkred"),lwd=5)

```

## Continuous Distributions II

\begin{description}

  \item[Gamma distribution:] $f(x|\alpha, \beta)= \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1} e^{-\beta x}$ is the density of the Gamma distribution denoted as $Ga(\alpha,\beta)$, where $x \geq 0$, the shape parameter $\alpha \in \mathbb{R}$ and the scale parameter $\beta \in \mathbb{R}^+$.
  \item[Exponential distribution:] $f(x|\lambda)= \lambda e^{-\lambda x}$ is the density of the exponential distribution denoted as $Exp(\lambda)$, where $x \geq 0$ and $\lambda \in \mathbb{R}^+$.
  \item[Chi-square distribution:] $f(x| k) = \frac{x^{k/2-1}e^{-x/2}}{2^{k/2} \Gamma(k/2)} $ is the density of the chi-square distribution with $k$ degrees of freedom, denoted as $\chi^2_k$, where $x \geq 0$ and $k \in \mathbb{N}^+$.

  \item[Pareto distribution:] $f(x| \alpha, x_0) = \alpha x_0^\alpha x^{-(\alpha+1)} $ is the density of the Pareto (Type I) distribution with tail index $\alpha$, denoted as $Pa(\alpha,x_0)$, where $x \geq x_0$ and $\alpha \in \mathbb{R}^+$.
\end{description}

## Continuous Distributions II - Visualisation

```{r distributionsII,echo=FALSE,fig.align='center',fig.height=10,fig.width=10,out.width=250}
library(Pareto)
x1<-seq(0,10,by=0.02)
x2<-seq(0,1,by=0.01)
ygamma1<-dgamma(x1,shape = 2,scale = 2)
ygamma2<-dgamma(x1,shape = 1,scale = 10)
yexp2<-dexp(x1,rate = 2)
yexp10<-dexp(x1,rate = 10)
ychisq<-dchisq(x1,df=2)
ychisq2<-dchisq(x1,df=5)
ypareto<-dPareto(x1,t = 1,alpha=2)
ypareto2<-dPareto(x1,t = 4,alpha=4)
#ybeta3<-dbeta(x2,shape1=2,shape2 = 2)
par(mfrow=c(2,2))
plot(x1,ygamma1,type="l",las=1,col="darkgreen",lwd=3,main="Gamma distribution",ylab="",xlab="x")
lines(x1,ygamma2,col="blue",lwd=3)
legend("topright",legend = c("shape=2,  scale=2","shape=1,scale=10"),col=c("darkgreen","blue"),lwd=5)
plot(x1,yexp10,type="l",las=1,col="blue",lwd=3,main="exponential distribution",ylab="",xlab="x",xlim=c(0,2))
lines(x1,yexp2,col="darkgreen",lwd=3)
legend("topright",legend = c("lambda=2","lambda=10"),col=c("darkgreen","blue"),lwd=5)
plot(x1,ychisq,type="l",las=1,col="darkgreen",lwd=3,main=paste0("Chi-Squared ",expression(chi^2)," distribution"),ylab="",xlab="x")
lines(x1,ychisq2,col="blue",lwd=3)
legend("topright",legend = c("df=2","df=5"),col=c("darkgreen","blue"),lwd=5)
plot(x1,ypareto,type="l",las=1,col="darkgreen",lwd=3,ylim=c(0,1.6),main="Pareto distribution",ylab="",xlab="x")
lines(x1,ypareto2,col="blue",lwd=3)
#lines(x2,ybeta3,col="darkred",lwd=3)
legend("top",legend = c("t=1, alpha=1","t=4, alpha=4"),col=c("darkgreen","blue"),lwd=5)

```

## Relations between Distributions visualised 

![Source: www.math.wm.edu/~leemis/2008amstat.pdf](Relationships_among_some_of_univariate_probability_distributions.jpg)

